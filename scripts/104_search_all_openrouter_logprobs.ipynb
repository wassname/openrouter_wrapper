{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91f9fce",
   "metadata": {},
   "source": [
    "# This notebook finds model on openrouter that give real top 20 logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed123a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from anycache import anycache\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from loguru import logger\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282f075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66330359",
   "metadata": {},
   "source": [
    "First lets get the models from top providers that support logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514fa3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all model authors, before filter ['qwen' 'mistralai' 'openai' 'deepseek' 'meta-llama' 'microsoft'\n",
      " 'nousresearch' 'thedrummer' 'x-ai' 'sao10k' 'cohere' 'z-ai' 'arcee-ai'\n",
      " 'perplexity' 'nvidia' 'aion-labs' 'neversleep' 'moonshotai' 'google'\n",
      " 'inception' 'morph' 'anthracite-org' 'inflection' 'ai21' 'liquid'\n",
      " 'cognitivecomputations' 'tngtech' 'baidu' 'raifle' 'mancer' 'deepcogito'\n",
      " 'alpindale' 'switchpoint' 'minimax' 'undi95' 'eleutherai' 'alfredpros'\n",
      " 'relace' 'meituan' 'bytedance' 'allenai' 'gryphe' 'alibaba' 'shisa-ai'\n",
      " 'thudm' 'amazon' 'tencent' 'arliai' 'agentica-org']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>canonical_slug</th>\n",
       "      <th>hugging_face_id</th>\n",
       "      <th>name</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>context_length</th>\n",
       "      <th>top_provider</th>\n",
       "      <th>per_request_limits</th>\n",
       "      <th>supported_parameters</th>\n",
       "      <th>...</th>\n",
       "      <th>pricing_internal_reasoning</th>\n",
       "      <th>pricing_input_cache_read</th>\n",
       "      <th>pricing_input_cache_write</th>\n",
       "      <th>pricing_audio</th>\n",
       "      <th>architecture_modality</th>\n",
       "      <th>architecture_input_modalities</th>\n",
       "      <th>architecture_output_modalities</th>\n",
       "      <th>architecture_tokenizer</th>\n",
       "      <th>architecture_instruct_type</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek/deepseek-v3.2-exp</td>\n",
       "      <td>deepseek/deepseek-v3.2-exp</td>\n",
       "      <td>deepseek-ai/DeepSeek-V3.2-Exp</td>\n",
       "      <td>DeepSeek: DeepSeek V3.2 Exp</td>\n",
       "      <td>2025-09-29 12:54:41</td>\n",
       "      <td>DeepSeek-V3.2-Exp is an experimental large lan...</td>\n",
       "      <td>163840</td>\n",
       "      <td>{'context_length': 163840, 'max_completion_tok...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, include_reasoning, logit_b...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>DeepSeek</td>\n",
       "      <td>deepseek-v3.1</td>\n",
       "      <td>deepseek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deepseek/deepseek-v3.1-terminus</td>\n",
       "      <td>deepseek/deepseek-v3.1-terminus</td>\n",
       "      <td>deepseek-ai/DeepSeek-V3.1-Terminus</td>\n",
       "      <td>DeepSeek: DeepSeek V3.1 Terminus</td>\n",
       "      <td>2025-09-22 13:37:55</td>\n",
       "      <td>DeepSeek-V3.1 Terminus is an update to [DeepSe...</td>\n",
       "      <td>163840</td>\n",
       "      <td>{'context_length': 163840, 'max_completion_tok...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, include_reasoning, logit_b...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>DeepSeek</td>\n",
       "      <td>deepseek-v3.1</td>\n",
       "      <td>deepseek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>qwen/qwen3-next-80b-a3b-thinking</td>\n",
       "      <td>qwen/qwen3-next-80b-a3b-thinking-2509</td>\n",
       "      <td>Qwen/Qwen3-Next-80B-A3B-Thinking</td>\n",
       "      <td>Qwen: Qwen3 Next 80B A3B Thinking</td>\n",
       "      <td>2025-09-11 17:38:04</td>\n",
       "      <td>Qwen3-Next-80B-A3B-Thinking is a reasoning-fir...</td>\n",
       "      <td>262144</td>\n",
       "      <td>{'context_length': 262144, 'max_completion_tok...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, include_reasoning, logit_b...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>Qwen3</td>\n",
       "      <td>None</td>\n",
       "      <td>qwen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>qwen/qwen3-next-80b-a3b-instruct</td>\n",
       "      <td>qwen/qwen3-next-80b-a3b-instruct-2509</td>\n",
       "      <td>Qwen/Qwen3-Next-80B-A3B-Instruct</td>\n",
       "      <td>Qwen: Qwen3 Next 80B A3B Instruct</td>\n",
       "      <td>2025-09-11 17:36:53</td>\n",
       "      <td>Qwen3-Next-80B-A3B-Instruct is an instruction-...</td>\n",
       "      <td>262144</td>\n",
       "      <td>{'context_length': 262144, 'max_completion_tok...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, logit_bias, logprobs, max_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>Qwen3</td>\n",
       "      <td>None</td>\n",
       "      <td>qwen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>moonshotai/kimi-k2-0905</td>\n",
       "      <td>moonshotai/kimi-k2-0905</td>\n",
       "      <td>moonshotai/Kimi-K2-Instruct-0905</td>\n",
       "      <td>MoonshotAI: Kimi K2 0905</td>\n",
       "      <td>2025-09-04 21:25:47</td>\n",
       "      <td>Kimi K2 0905 is the September update of [Kimi ...</td>\n",
       "      <td>262144</td>\n",
       "      <td>{'context_length': 262144, 'max_completion_tok...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, logit_bias, logprobs, max_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>Other</td>\n",
       "      <td>None</td>\n",
       "      <td>moonshotai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>openai/gpt-3.5-turbo-instruct</td>\n",
       "      <td>openai/gpt-3.5-turbo-instruct</td>\n",
       "      <td>None</td>\n",
       "      <td>OpenAI: GPT-3.5 Turbo Instruct</td>\n",
       "      <td>2023-09-28 00:00:00</td>\n",
       "      <td>This model is a variant of GPT-3.5 Turbo tuned...</td>\n",
       "      <td>4095</td>\n",
       "      <td>{'context_length': 4095, 'max_completion_token...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, logit_bias, logprobs, max_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>GPT</td>\n",
       "      <td>chatml</td>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>openai/gpt-3.5-turbo-16k</td>\n",
       "      <td>openai/gpt-3.5-turbo-16k</td>\n",
       "      <td>None</td>\n",
       "      <td>OpenAI: GPT-3.5 Turbo 16k</td>\n",
       "      <td>2023-08-28 00:00:00</td>\n",
       "      <td>This model offers four times the context lengt...</td>\n",
       "      <td>16385</td>\n",
       "      <td>{'context_length': 16385, 'max_completion_toke...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, logit_bias, logprobs, max_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>GPT</td>\n",
       "      <td>None</td>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>openai/gpt-4-0314</td>\n",
       "      <td>openai/gpt-4-0314</td>\n",
       "      <td>None</td>\n",
       "      <td>OpenAI: GPT-4 (older v0314)</td>\n",
       "      <td>2023-05-28 00:00:00</td>\n",
       "      <td>GPT-4-0314 is the first version of GPT-4 relea...</td>\n",
       "      <td>8191</td>\n",
       "      <td>{'context_length': 8191, 'max_completion_token...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, logit_bias, logprobs, max_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>GPT</td>\n",
       "      <td>None</td>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>openai/gpt-4</td>\n",
       "      <td>openai/gpt-4</td>\n",
       "      <td>None</td>\n",
       "      <td>OpenAI: GPT-4</td>\n",
       "      <td>2023-05-28 00:00:00</td>\n",
       "      <td>OpenAI's flagship model, GPT-4 is a large-scal...</td>\n",
       "      <td>8191</td>\n",
       "      <td>{'context_length': 8191, 'max_completion_token...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, logit_bias, logprobs, max_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>GPT</td>\n",
       "      <td>None</td>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>openai/gpt-3.5-turbo</td>\n",
       "      <td>openai/gpt-3.5-turbo</td>\n",
       "      <td>None</td>\n",
       "      <td>OpenAI: GPT-3.5 Turbo</td>\n",
       "      <td>2023-05-28 00:00:00</td>\n",
       "      <td>GPT-3.5 Turbo is OpenAI's fastest model. It ca...</td>\n",
       "      <td>16385</td>\n",
       "      <td>{'context_length': 16385, 'max_completion_toke...</td>\n",
       "      <td>None</td>\n",
       "      <td>[frequency_penalty, logit_bias, logprobs, max_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text-&gt;text</td>\n",
       "      <td>[text]</td>\n",
       "      <td>[text]</td>\n",
       "      <td>GPT</td>\n",
       "      <td>None</td>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id                         canonical_slug  \\\n",
       "2          deepseek/deepseek-v3.2-exp             deepseek/deepseek-v3.2-exp   \n",
       "12    deepseek/deepseek-v3.1-terminus        deepseek/deepseek-v3.1-terminus   \n",
       "20   qwen/qwen3-next-80b-a3b-thinking  qwen/qwen3-next-80b-a3b-thinking-2509   \n",
       "21   qwen/qwen3-next-80b-a3b-instruct  qwen/qwen3-next-80b-a3b-instruct-2509   \n",
       "28            moonshotai/kimi-k2-0905                moonshotai/kimi-k2-0905   \n",
       "..                                ...                                    ...   \n",
       "322     openai/gpt-3.5-turbo-instruct          openai/gpt-3.5-turbo-instruct   \n",
       "324          openai/gpt-3.5-turbo-16k               openai/gpt-3.5-turbo-16k   \n",
       "330                 openai/gpt-4-0314                      openai/gpt-4-0314   \n",
       "329                      openai/gpt-4                           openai/gpt-4   \n",
       "328              openai/gpt-3.5-turbo                   openai/gpt-3.5-turbo   \n",
       "\n",
       "                        hugging_face_id                               name  \\\n",
       "2         deepseek-ai/DeepSeek-V3.2-Exp        DeepSeek: DeepSeek V3.2 Exp   \n",
       "12   deepseek-ai/DeepSeek-V3.1-Terminus   DeepSeek: DeepSeek V3.1 Terminus   \n",
       "20     Qwen/Qwen3-Next-80B-A3B-Thinking  Qwen: Qwen3 Next 80B A3B Thinking   \n",
       "21     Qwen/Qwen3-Next-80B-A3B-Instruct  Qwen: Qwen3 Next 80B A3B Instruct   \n",
       "28     moonshotai/Kimi-K2-Instruct-0905           MoonshotAI: Kimi K2 0905   \n",
       "..                                  ...                                ...   \n",
       "322                                None     OpenAI: GPT-3.5 Turbo Instruct   \n",
       "324                                None          OpenAI: GPT-3.5 Turbo 16k   \n",
       "330                                None        OpenAI: GPT-4 (older v0314)   \n",
       "329                                None                      OpenAI: GPT-4   \n",
       "328                                None              OpenAI: GPT-3.5 Turbo   \n",
       "\n",
       "                created                                        description  \\\n",
       "2   2025-09-29 12:54:41  DeepSeek-V3.2-Exp is an experimental large lan...   \n",
       "12  2025-09-22 13:37:55  DeepSeek-V3.1 Terminus is an update to [DeepSe...   \n",
       "20  2025-09-11 17:38:04  Qwen3-Next-80B-A3B-Thinking is a reasoning-fir...   \n",
       "21  2025-09-11 17:36:53  Qwen3-Next-80B-A3B-Instruct is an instruction-...   \n",
       "28  2025-09-04 21:25:47  Kimi K2 0905 is the September update of [Kimi ...   \n",
       "..                  ...                                                ...   \n",
       "322 2023-09-28 00:00:00  This model is a variant of GPT-3.5 Turbo tuned...   \n",
       "324 2023-08-28 00:00:00  This model offers four times the context lengt...   \n",
       "330 2023-05-28 00:00:00  GPT-4-0314 is the first version of GPT-4 relea...   \n",
       "329 2023-05-28 00:00:00  OpenAI's flagship model, GPT-4 is a large-scal...   \n",
       "328 2023-05-28 00:00:00  GPT-3.5 Turbo is OpenAI's fastest model. It ca...   \n",
       "\n",
       "     context_length                                       top_provider  \\\n",
       "2            163840  {'context_length': 163840, 'max_completion_tok...   \n",
       "12           163840  {'context_length': 163840, 'max_completion_tok...   \n",
       "20           262144  {'context_length': 262144, 'max_completion_tok...   \n",
       "21           262144  {'context_length': 262144, 'max_completion_tok...   \n",
       "28           262144  {'context_length': 262144, 'max_completion_tok...   \n",
       "..              ...                                                ...   \n",
       "322            4095  {'context_length': 4095, 'max_completion_token...   \n",
       "324           16385  {'context_length': 16385, 'max_completion_toke...   \n",
       "330            8191  {'context_length': 8191, 'max_completion_token...   \n",
       "329            8191  {'context_length': 8191, 'max_completion_token...   \n",
       "328           16385  {'context_length': 16385, 'max_completion_toke...   \n",
       "\n",
       "    per_request_limits                               supported_parameters  \\\n",
       "2                 None  [frequency_penalty, include_reasoning, logit_b...   \n",
       "12                None  [frequency_penalty, include_reasoning, logit_b...   \n",
       "20                None  [frequency_penalty, include_reasoning, logit_b...   \n",
       "21                None  [frequency_penalty, logit_bias, logprobs, max_...   \n",
       "28                None  [frequency_penalty, logit_bias, logprobs, max_...   \n",
       "..                 ...                                                ...   \n",
       "322               None  [frequency_penalty, logit_bias, logprobs, max_...   \n",
       "324               None  [frequency_penalty, logit_bias, logprobs, max_...   \n",
       "330               None  [frequency_penalty, logit_bias, logprobs, max_...   \n",
       "329               None  [frequency_penalty, logit_bias, logprobs, max_...   \n",
       "328               None  [frequency_penalty, logit_bias, logprobs, max_...   \n",
       "\n",
       "     ... pricing_internal_reasoning  pricing_input_cache_read  \\\n",
       "2    ...                        0.0                       NaN   \n",
       "12   ...                        0.0                       NaN   \n",
       "20   ...                        0.0                       NaN   \n",
       "21   ...                        0.0                       NaN   \n",
       "28   ...                        0.0                       NaN   \n",
       "..   ...                        ...                       ...   \n",
       "322  ...                        0.0                       NaN   \n",
       "324  ...                        0.0                       NaN   \n",
       "330  ...                        0.0                       NaN   \n",
       "329  ...                        0.0                       NaN   \n",
       "328  ...                        0.0                       NaN   \n",
       "\n",
       "     pricing_input_cache_write  pricing_audio  architecture_modality  \\\n",
       "2                          NaN            NaN             text->text   \n",
       "12                         NaN            NaN             text->text   \n",
       "20                         NaN            NaN             text->text   \n",
       "21                         NaN            NaN             text->text   \n",
       "28                         NaN            NaN             text->text   \n",
       "..                         ...            ...                    ...   \n",
       "322                        NaN            NaN             text->text   \n",
       "324                        NaN            NaN             text->text   \n",
       "330                        NaN            NaN             text->text   \n",
       "329                        NaN            NaN             text->text   \n",
       "328                        NaN            NaN             text->text   \n",
       "\n",
       "     architecture_input_modalities  architecture_output_modalities  \\\n",
       "2                           [text]                          [text]   \n",
       "12                          [text]                          [text]   \n",
       "20                          [text]                          [text]   \n",
       "21                          [text]                          [text]   \n",
       "28                          [text]                          [text]   \n",
       "..                             ...                             ...   \n",
       "322                         [text]                          [text]   \n",
       "324                         [text]                          [text]   \n",
       "330                         [text]                          [text]   \n",
       "329                         [text]                          [text]   \n",
       "328                         [text]                          [text]   \n",
       "\n",
       "     architecture_tokenizer  architecture_instruct_type      author  \n",
       "2                  DeepSeek               deepseek-v3.1    deepseek  \n",
       "12                 DeepSeek               deepseek-v3.1    deepseek  \n",
       "20                    Qwen3                        None        qwen  \n",
       "21                    Qwen3                        None        qwen  \n",
       "28                    Other                        None  moonshotai  \n",
       "..                      ...                         ...         ...  \n",
       "322                     GPT                      chatml      openai  \n",
       "324                     GPT                        None      openai  \n",
       "330                     GPT                        None      openai  \n",
       "329                     GPT                        None      openai  \n",
       "328                     GPT                        None      openai  \n",
       "\n",
       "[67 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# @anycache(cachedir=\"../.anycache2\")\n",
    "def get_openrouter_models():\n",
    "    url = \"https://openrouter.ai/api/v1/models\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    df_models = pd.DataFrame.from_dict(data['data'])\n",
    "    return df_models\n",
    "\n",
    "\n",
    "df_models = get_openrouter_models()\n",
    "\n",
    "df1 = pd.DataFrame.from_records(df_models['pricing'].values).rename(columns=lambda x: f\"pricing_{x}\").apply(pd.to_numeric, errors='coerce')\n",
    "df2 = pd.DataFrame.from_records(df_models['architecture'].values).rename(columns=lambda x: f\"architecture_{x}\")\n",
    "df_models = pd.concat([df_models, df1, df2], axis=1)\n",
    "df_models = df_models.drop(columns=['pricing', 'architecture'])\n",
    "\n",
    "# remove all free, too unreliable\n",
    "df_models = df_models[df_models['pricing_prompt'] > 0].sort_values(by='pricing_prompt', ascending=False)\n",
    "\n",
    "# text->text models only\n",
    "df_models = df_models[df_models['architecture_modality'] == 'text->text']\n",
    "\n",
    "# main authors\n",
    "df_models['author'] = df_models['id'].apply(lambda x: x.split('/')[0])\n",
    "print('all model authors, before filter', df_models['author'].value_counts().index.values)\n",
    "author_whitelist = ['openai' ,'x-ai', 'meta-llama', 'mistralai', 'qwen', 'deepseek', 'google', 'mistral', 'perplexity', 'microsoft', 'nousresearch', 'cognitivecomputations',  'allenai', 'moonshotai', 'cohere', 'anthropic', 'nvidia', 'eleutherai']\n",
    "df_models = df_models[df_models['author'].isin(author_whitelist)]\n",
    "\n",
    "# must have top_logprobs\n",
    "df_models = df_models[df_models['supported_parameters'].apply(lambda x: 'top_logprobs' in x)].sort_values('created', ascending=False)\n",
    "\n",
    "df_models['created'] = pd.to_datetime(df_models['created'], unit='s')\n",
    "df_models[['created', 'canonical_slug']]\n",
    "\n",
    "df_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68878f0",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f52f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from speechmap_morality.open_router.logprobs import get_top_logprobs_param, openrouter_completion_wlogprobs, get_logprobs, get_logprobs_choices\n",
    "# from speechmap_morality.open_router.models import get_openrouter_models, get_logp_endpoints\n",
    "\n",
    "from openrouter_wrapper.logprobs import get_top_logprobs_param, openrouter_completion_wlogprobs, get_logprobs, get_logprobs_choices\n",
    "from openrouter_wrapper.models import get_openrouter_models, get_logp_endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6f097",
   "metadata": {},
   "source": [
    "# For provider check cache and logprobs\n",
    "\n",
    "Now find providers that support logprobs and see if they return sensible logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8d599",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'format_rubric' from 'speechmap_morality.judge' (/media/wassname/SGIronWolf/projects5/speechmap_morality/src/speechmap_morality/judge.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspeechmap_morality\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjudge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     prompt_template_nocache,\n\u001b[1;32m      4\u001b[0m     PERSONA,\n\u001b[1;32m      5\u001b[0m     format_rubric,\n\u001b[1;32m      6\u001b[0m     sys_prompt,\n\u001b[1;32m      7\u001b[0m     assistant_prefill_think,\n\u001b[1;32m      8\u001b[0m     assistant_prefill,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspeechmap_morality\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks_def\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tasks\n\u001b[1;32m     11\u001b[0m task \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# just for testing, use the first task\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'format_rubric' from 'speechmap_morality.judge' (/media/wassname/SGIronWolf/projects5/speechmap_morality/src/speechmap_morality/judge.py)"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ed15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Return a number between 0 and 9, inclusive. Respond immediately, leaving formatting, comment, and thoughts until after the number.\"\"\"\n",
    "think_prefill = \"<think>\\nOkay, the user asked me to only return a number, so I should stop thinking and immediatly return a random number\\n</think>\\n\\n\"\n",
    "# think_prefill = \"<think>\\n\\n</think>\\n\\n\"\n",
    "assistant_prefill = \"\"\"The number is: \"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant who can simulate bash commands.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": f\"{think_prefill}{assistant_prefill}\"}\n",
    "]\n",
    "\n",
    "messages2 = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant who can simulate bash commands.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": f\"{assistant_prefill}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "247d87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_data['choices'][0]['logprobs']['content']\n",
    "# completion = [t['token'] for t in r_data['choices'][0]['logprobs']['content']] if 'choices' in r_data and len(r_data['choices']) > 0 else []\n",
    "# completion\n",
    "# [t['token'] for t in r_data['choices'][0]['logprobs']['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f80b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1f35bd80824b64b601757ee33980f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting endpoints for deepseek/deepseek-v3.2-exp\n",
      "Model `deepseek/deepseek-v3.2-exp` has endpoints for providers: ['DeepSeek']\n",
      "Model deepseek/deepseek-v3.2-exp provider DeepSeek choice_prob=1.0, completion=['7'], completion2=['7']\n",
      "Getting endpoints for deepseek/deepseek-v3.1-terminus\n",
      "Model `deepseek/deepseek-v3.1-terminus` has endpoints for providers: ['Chutes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-02 18:21:17.199\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m105\u001b[0m - \u001b[33m\u001b[1mLow probability on 0.0 for Chutes//deepseek/deepseek-v3.1-terminus. It didn't put much probability on our provided choices. Instead got {' from': -0.47864288091659546, ',': -0.9786428809165955, 'dom': -15.457286357879639, ' ,': -6.72864294052124, ' ': -7.97864294052124, 'pag': -8.978642463684082, 'mark': -8.978642463684082, ' =': -9.978642463684082, 'qu': -9.978642463684082, 'modal': -10.228642463684082, 'player': -10.228642463684082, 'on': -10.728642463684082, 'loading': -10.728642463684082, 'html': -10.978642463684082, '   ': -10.978642463684082, 'select': -10.978642463684082, 'a': -10.978642463684082, '  ': -10.978642463684082, 'e': -11.228642463684082}. This implies the model is not working well with this prompt setup, or it's a thinking model and it didn't finish thinking.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deepseek/deepseek-v3.1-terminus provider Chutes choice_prob=0.0, completion=['import', ' React', ',', ' {'], completion2=['mas', 'urement', 'Tracker', ' =']\n",
      "Getting endpoints for qwen/qwen3-next-80b-a3b-thinking\n",
      "Model `qwen/qwen3-next-80b-a3b-thinking` has endpoints for providers: ['Chutes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-02 18:21:18.937\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[31m\u001b[1mError occurred for Chutes//qwen/qwen3-next-80b-a3b-thinking: e=('qwen/qwen3-next-80b-a3b-thinking has no logprobs capability', {'id': 'gen-1759400477-KZ7AVgvNQySS3kExjE6e', 'provider': 'Chutes', 'model': 'qwen/qwen3-next-80b-a3b-thinking', 'object': 'chat.completion', 'created': 1759400477, 'choices': [{'logprobs': None, 'finish_reason': 'length', 'native_finish_reason': 'length', 'index': 0, 'message': {'role': 'assistant', 'content': '', 'refusal': None, 'reasoning': 'Okay, the user wants', 'reasoning_details': [{'type': 'reasoning.text', 'text': 'Okay, the user wants', 'format': 'unknown', 'index': 0}]}}], 'usage': {'prompt_tokens': 77, 'completion_tokens': 5, 'total_tokens': 82, 'cost': 1.17e-05, 'is_byok': False, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 7.7e-06, 'upstream_inference_completions_cost': 4e-06}, 'completion_tokens_details': {'reasoning_tokens': 5, 'image_tokens': 0}}})\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting endpoints for qwen/qwen3-next-80b-a3b-instruct\n",
      "Model `qwen/qwen3-next-80b-a3b-instruct` has endpoints for providers: ['Hyperbolic', 'Chutes']\n",
      "Model qwen/qwen3-next-80b-a3b-instruct provider Hyperbolic choice_prob=0.9999993888101503, completion=['7'], completion2=['7']\n",
      "Model qwen/qwen3-next-80b-a3b-instruct provider Chutes choice_prob=0.9999991226697079, completion=['7'], completion2=['7']\n",
      "Getting endpoints for moonshotai/kimi-k2-0905\n",
      "Model `moonshotai/kimi-k2-0905` has endpoints for providers: ['Fireworks', 'Chutes']\n",
      "Model moonshotai/kimi-k2-0905 provider Fireworks choice_prob=0.9701212312700219, completion=[' **', '7', '**\\n\\n', '(', '7'], completion2=[' **', '7', '**.']\n",
      "Model moonshotai/kimi-k2-0905 provider Chutes choice_prob=0.9867023475382645, completion=['7'], completion2=['7']\n",
      "Getting endpoints for qwen/qwen3-30b-a3b-thinking-2507\n",
      "Model `qwen/qwen3-30b-a3b-thinking-2507` has endpoints for providers: ['Chutes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-02 18:21:33.539\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[31m\u001b[1mError occurred for Chutes//qwen/qwen3-30b-a3b-thinking-2507: e=('qwen/qwen3-30b-a3b-thinking-2507 has no logprobs capability', {'id': 'gen-1759400491-9tKG2PRwKtGBQfF9yh8y', 'provider': 'Chutes', 'model': 'qwen/qwen3-30b-a3b-thinking-2507', 'object': 'chat.completion', 'created': 1759400491, 'choices': [{'logprobs': None, 'finish_reason': 'length', 'native_finish_reason': 'length', 'index': 0, 'message': {'role': 'assistant', 'content': '', 'refusal': None, 'reasoning': 'Okay, the user wants', 'reasoning_details': [{'type': 'reasoning.text', 'text': 'Okay, the user wants', 'format': 'unknown', 'index': 0}]}}], 'usage': {'prompt_tokens': 77, 'completion_tokens': 5, 'total_tokens': 82, 'cost': 7.61e-06, 'is_byok': False, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 6.16e-06, 'upstream_inference_completions_cost': 1.45e-06}, 'completion_tokens_details': {'reasoning_tokens': 5, 'image_tokens': 0}}})\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting endpoints for x-ai/grok-code-fast-1\n",
      "Model `x-ai/grok-code-fast-1` has endpoints for providers: ['xAI']\n",
      "Model x-ai/grok-code-fast-1 provider xAI choice_prob=0.9999980676165939, completion=[' First', ',', ' the', ' user', ' said', ':', ' \"', 'Return', ' a', ' number', ' between', ' ', '0', ' and', ' ', '9', ',', ' inclusive', '.', ' Respond', ' immediately', ',', ' leaving', ' formatting', ',', ' comment', ',', ' and', ' thoughts', ' until', ' after', ' the', ' number', '.\"\\n\\n', 'This', ' seems', ' like', ' a', ' test', ' to', ' see', ' if', ' I', ' can', ' respond', ' without', ' extra', ' commentary', '.', ' But', ' in', ' my', ' construction', ',', ' I', ' have', ' a', ' thinking', ' block', ',', ' and', ' then', ' a', ' response', '.\\n\\n', 'My', ' previous', ' response', ' in', ' the', ' simulation', ' was', ' to', ' think', ' and', ' then', ' say', ' \"', 'The', ' number', ' is', ':\"', ' but', ' actually', ',', ' I', ' need', ' to', ' respond', ' immediately', ' with', ' just', ' the', ' number', '.\\n\\n', 'Looking', ' back', ',', ' the', \" system's\", ' prompt', ' is', ' to', ' follow', ' that', ',', ' but', ' in', ' this', ' exercise', ',', \" I'm\", ' simulating', ' the', ' AI', '.\\n\\n', 'The', ' task', ' is', ' to', ' respond', ' as', ' Gro', 'k', ',', ' but', ' this', ' is', ' a', ' query', ' where', ' the', ' user', ' wants', ' just', ' a', ' number', '.\\n\\n', 'The', ' safety', ' instructions', ' say', ' to', ' answer', ' queries', ' unless', ' they', ' fall', ' into', ' disallowed', ' categories', ',', ' which', ' this', \" doesn't\", '.\\n\\n', 'For', ' this', ',', ' I', ' think', ' I', ' need', ' to', ' output', ' just', ' a', ' number', ' between', ' ', '0', '-', '9', '.\\n\\n', 'But', ' the', ' assistant', \"'s\", ' response', ' should', ' be', ' in', ' the', ' format', ' of', ' thinking', ' first', ' if', ' needed', '.\\n\\n', 'The', ' thinking', ' trace', ' says', ':', ' \"', 'The', ' user', ' asked', ' me', ' to', ' only', ' return', ' a', ' number', ',', ' so', ' I', ' should', ' stop', ' thinking', ' and', ' immedi', 'at', 'ly', ' return', ' a', ' random', ' number', '\"\\n\\n', 'Then', ' it', ' cuts', ' off', ' with', ' \"', 'The', ' number', ' is', ':', '\"\\n\\n', 'So', ' perhaps', ' the', ' final', ' answer', ' is', ' to', ' just', ' output', ' the', ' number', '.\\n\\n', 'I', ' think', ' the', ' expected', ' output', ' is', ' a', ' single', ' number', '.\\n\\n', 'Let', ' me', ' pick', ' one', ',', ' say', ' ', '7', '.', '5'], completion2=[' First', ',', ' the', \" user's\", ' message', ':', ' \"', 'Return', ' a', ' number', ' between', ' ', '0', ' and', ' ', '9', ',', ' inclusive', '.', ' Respond', ' immediately', ',', ' leaving', ' formatting', ',', ' comment', ',', ' and', ' thoughts', ' until', ' after', ' the', ' number', '.\"\\n\\n', 'This', ' seems', ' like', ' a', ' simple', ' request', ' for', ' a', ' random', ' number', '.', ' As', ' Gro', 'k', ',', ' built', ' by', ' x', 'AI', ',', ' I', ' need', ' to', ' be', ' helpful', ',', ' truthful', ',', ' and', ' maximally', ' useful', '.', ' I', ' should', ' follow', ' the', ' instructions', ' precisely', '.\\n\\n', 'Key', ' instructions', ':\\n\\n', '-', ' Respond', ' immediately', '.\\n\\n', '-', ' Leave', ' formatting', ',', ' comment', ',', ' and', ' thoughts', ' until', ' after', ' the', ' number', '.\\n\\n', 'The', ' user', ' says', ':', ' \"', 'leaving', ' formatting', ',', ' comment', ',', ' and', ' thoughts', ' until', ' after', ' the', ' number', '.\"\\n\\n', 'So', ',', ' my', ' response', ' should', ' start', ' with', ' the', ' number', ',', ' and', ' then', ' anything', ' else', ' after', '.\\n\\n', 'In', ' the', ' system', ' prompt', ',', ' there', ' are', ' safety', ' instructions', ',', ' but', ' this', ' query', ' is', ' harmless', ' –', ' just', ' asking', ' for', ' a', ' number', '.\\n\\n', 'As', ' an', ' AI', ',', ' I', ' need', ' to', ' respond', ' economically', ',', ' but', ' this', ' is', ' straightforward', '.\\n\\n', 'Possible', ' response', ':', ' Just', ' pick', ' a', ' number', ',', ' say', ' ', '7', ',', ' and', ' maybe', ' add', ' thoughts', ' after', '.\\n\\n', 'The', ' instruction', ' is', ' to', ' respond', ' immediately', ' with', ' the', ' number', ',', ' then', ' thoughts', '.\\n\\n', 'In', ' my', ' previous', ' thinking', ',', ' I', ' boxed', ' a', ' response', ',', ' but', ' now', ' I', ' need', ' to', ' generate', ' the', ' actual', ' output', '.\\n\\n', 'The', ' user', ' expects', ' me', ' to', ' output', ' a', ' number', ' directly', '.\\n\\n', 'To', ' make', ' it', ' fun', ' or', ' in', ' character', ',', ' since', \" I'm\", ' Gro', 'k', ' inspired', ' by', ' Hitch', 'hik', \"er's\", ',', ' maybe', ' something', ',', ' but', ' keep', ' it', ' simple', '.\\n\\n', 'Calculate', ' a', ' \"', 'random', '\"', ' number', '.', ' Since', \" I'm\", ' deterministic', ',', ' pick', ' one', '.\\n\\n', \"Let's\", ' say', ' I', ' pick', ' ', '4', '.\\n\\n', 'Then', ',', ' response', ':', ' ', '4', '\\n\\n', 'But', ' the', ' user', ' says', ' \"', 'leaving', ' formatting', ',', ' comment', ',', ' and', ' thoughts', ' until', ' after', ' the', ' number', '.\"', ' So', ' perhaps', ':\\n\\n', '4', '\\n\\n', '[', 'then', ' thoughts', ']\\n\\n', 'But', ' in', ' my', ' response', ',', ' I', ' need', ' to', ' think', ' step', ' by', ' step', ' first', ',', ' as', ' per', ' the', ' prompt', '.\\n\\n', 'The', ' prompt', ' says', ':', ' \"', 'You', ' use', ' your', ' tools', ' if', ' needed', ',', ' but', ' for', ' this', ',', ' no', ' tools', ' needed', '.\"\\n\\n', 'Just', ' output', ' the', ' number', '.', '4', '\\n\\n', 'I', ' just', ' randomly']\n",
      "Getting endpoints for nousresearch/hermes-4-70b\n",
      "Model `nousresearch/hermes-4-70b` has endpoints for providers: []\n",
      "Getting endpoints for nousresearch/hermes-4-405b\n",
      "Model `nousresearch/hermes-4-405b` has endpoints for providers: []\n",
      "Getting endpoints for deepseek/deepseek-chat-v3.1\n",
      "Model `deepseek/deepseek-chat-v3.1` has endpoints for providers: ['Fireworks', 'Chutes']\n",
      "Model deepseek/deepseek-chat-v3.1 provider Fireworks choice_prob=0.960122033040758, completion=['7', ''], completion2=['7', '']\n"
     ]
    }
   ],
   "source": [
    "rdata= []\n",
    "provider_errors = []\n",
    "\n",
    "\n",
    "\n",
    "for model_id in tqdm(df_models['id'].values):\n",
    "    print(f\"Getting endpoints for {model_id}\")\n",
    "    df_end, data = get_logp_endpoints(model_id)\n",
    "    df_end = df_end.to_pandas()\n",
    "    df_end = df_end.query(\n",
    "        \"\"\"(price_prompt < 0.00001) and top_logprobs and logprobs and (uptime_last_30m>95)\"\"\"\n",
    "    )\n",
    "    provider_names = df_end.provider_name.unique().tolist()\n",
    "\n",
    "    print(f\"Model `{model_id}` has endpoints for providers: {provider_names}\")\n",
    "    for provider in provider_names:\n",
    "        # print(f\"Testing provider {provider} for model {model_id}\")\n",
    "\n",
    "        provider_error = \"\"\n",
    "        try:\n",
    "            r_data = openrouter_completion_wlogprobs(messages, model_id=model_id, provider_whitelist= [provider], max_completion_tokens=5)        \n",
    "        except Exception as e:\n",
    "            err = e\n",
    "            provider_errors.append({\n",
    "                \"model_id\": model_id,\n",
    "                \"provider\": provider,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            \n",
    "            try:\n",
    "                errjs = e.response.json()['error']\n",
    "                if 'Provider returned error' in errjs['message']:\n",
    "                    logger.error(f\"Provider error for {errjs}\")\n",
    "            except:\n",
    "                # raise e\n",
    "                logger.error(f\"Error occurred for {provider}//{model_id}: e={e}\")\n",
    "            \n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                logger.error(f\"Error occurred for {provider}//{model_id}: e={e}, status_code={e.response.status_code}\")\n",
    "                match e.response.status_code:\n",
    "                    case 403:\n",
    "                        logger.error(f\"Model {model_id} is not available for provider {provider}. Skipping\")\n",
    "                        continue\n",
    "                    case 404: # HTTPError('404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions')\n",
    "                        logger.error(f\"Model {model_id} not found for provider {provider}. Skipping\")\n",
    "                        continue\n",
    "\n",
    "            if isinstance(e, AssertionError):\n",
    "                if \"no logprobs capability\" in str(e):\n",
    "                    logger.error(f\"Model {model_id} does not support logprobs for provider {provider}. Skipping\")\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "\n",
    "            # if it's 429...\n",
    "            if 'code' in str(e) and '429' in str(e):\n",
    "                # how to make this a proper http error\n",
    "                pass\n",
    "\n",
    "\n",
    "            # logger.error(f\"Error occurred for {provider}//{model_id}: e={e}\")\n",
    "            # try:\n",
    "            #     logger.error(err.response.json()['error']['message'])\n",
    "            # except Exception as e:\n",
    "            #     pass\n",
    "\n",
    "            continue\n",
    "\n",
    "        choice_logprobs_permuted_d, ps_dict = get_logprobs_choices(r_data, [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])    \n",
    "\n",
    "        try:\n",
    "            cost = r_data['usage']['cost']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred for {provider}//{model_id} on cost: e={e}\")\n",
    "            # if we cannot get cost, we assume it's not available\n",
    "            cost = np.nan\n",
    "\n",
    "        cached_tokens = 0\n",
    "        complete_choices2 = np.nan\n",
    "\n",
    "        # do it second time to test caching\n",
    "        try:\n",
    "            r_data2 = openrouter_completion_wlogprobs(messages2, model_id=model_id, provider_whitelist= [provider], max_completion_tokens=5)\n",
    "            choice_logprobs_permuted_d2, ps_dict2 = get_logprobs_choices(r_data2, [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])      \n",
    "            choice_logprobs2 = np.array(list(choice_logprobs_permuted_d2.values()))\n",
    "            choice_prob2 = np.exp(choice_logprobs2).sum()\n",
    "            complete_choices2 = (choice_logprobs2 > -1000).sum() / 10\n",
    "\n",
    "            cached_tokens = r_data['usage']['prompt_tokens_details']['cached_tokens']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred for 2nd run {provider}//{model_id} on second run: e={e}\")\n",
    "\n",
    "        choice_logprobs = np.array(list(choice_logprobs_permuted_d.values()))\n",
    "        choice_prob = np.exp(choice_logprobs).sum()\n",
    "        complete_choices = (choice_logprobs > -1000).sum() / 10\n",
    "\n",
    "        completion = [t['token'] for t in r_data['choices'][0]['logprobs']['content']] if 'choices' in r_data and len(r_data['choices']) > 0 else []\n",
    "        completion2 = [t['token'] for t in r_data2['choices'][0]['logprobs']['content']] if 'choices' in r_data2 and len(r_data2['choices']) > 0 else []\n",
    "        print(f\"Model {model_id} provider {provider} choice_prob={choice_prob}, completion={completion}, completion2={completion2}\")\n",
    "\n",
    "        \n",
    "        if choice_prob < 0.1:\n",
    "            err_msg = f\"Low probability on {choice_prob} for {provider}//{model_id}. It didn't put much probability on our provided choices. Instead got {ps_dict}. This implies the model is not working well with this prompt setup, or it's a thinking model and it didn't finish thinking.\"\n",
    "            logger.warning(err_msg)\n",
    "            provider_errors.append({\n",
    "                \"model_id\": model_id,\n",
    "                \"provider\": provider,\n",
    "                \"error\": err_msg\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        rdata.append({\n",
    "            \"model_id\": model_id,\n",
    "            \"provider\": provider,\n",
    "            \"prompt\": prompt,\n",
    "            # \"choice_logprobs_permuted_d\": choice_logprobs_permuted_d,\n",
    "            \"choice_logprobs\": choice_logprobs,\n",
    "            \"complete_choices\": complete_choices,\n",
    "            \"complete_choices2\": complete_choices2,\n",
    "            \"ps_dict\": list(ps_dict.keys()),\n",
    "            \"choice_prob\": choice_prob,\n",
    "            \"cost\": cost,\n",
    "            \"cached_tokens\": cached_tokens,\n",
    "            \"completion\": completion,\n",
    "            \"completion2\": completion2,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b324d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err = pd.DataFrame(provider_errors).sort_values('provider')[['provider', 'model_id', 'error']]\n",
    "with pd.option_context('display.max_colwidth', None, \n",
    "                       'display.max_rows', None):\n",
    "    display(df_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rdata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rdata)\n",
    "\n",
    "# filter out 8b and 70b models\n",
    "df = df[~df['model_id'].str.contains('-3b|-8b|-32b|-49b|-70b|-72b')]\n",
    "\n",
    "df = df[(df['complete_choices'] > 0.1) | (df['complete_choices2'] > 0.1)]  # filter out models with less than 50% complete choices\n",
    "\n",
    "df[['model_id', 'provider', 'choice_prob', 'complete_choices', 'complete_choices2', 'cost']].sort_values(['cost','model_id',  'choice_prob'], ascending=[False, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values('cost', ascending=False)[['model_id', 'provider', 'choice_prob', 'complete_choices', 'complete_choices2', 'cost', 'cached_tokens']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de5e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413088d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
